{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия (максимум 10 баллов)\n",
    "\n",
    "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQjq1AU49Q-q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# типо я умная \n",
    "surname = \"Хачатрян\"\n",
    "letters_count = len(surname) \n",
    "variant = (letters_count % 4) + 1\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заранее спасибо за проверку! Хочу отметить, что на протяжении всей работы я использовала ChatGPT для следующих целей:\n",
    "\n",
    "- Редактирование выводов, чтобы они звучали более красиво и убедительно\n",
    "- Оптимизация и улучшение кода (я отметила в коде)\n",
    "- Объяснение всего, что я не понимала из дано (ну этого было меньше, но все же)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FsuljEb9Q-w"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChBks_AF9Q-y"
   },
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jihDOyOd9Q-0"
   },
   "source": [
    "Создадим набор данных для многомерной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "ykkouQff9Q-2",
    "outputId": "8223f891-c775-432e-c125-53bff94c60c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMZP4pwT9Q-3"
   },
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "a-cl2ujU9Q-4",
    "outputId": "dc936815-cd0a-4c42-bc79-824337410051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7078495982110444e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IKaW67f9Q-6"
   },
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Fzw6etrW9Q-7",
    "outputId": "9a1effb5-b3d9-47cc-bd8b-c696505c8521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.335882406413006e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.96351787e-08,  9.70832847e+01, -1.59045498e-08,  4.39528809e+01,\n",
       "        2.88159577e-08,  1.51935039e-08,  6.21926360e+00, -1.32623582e-08,\n",
       "        1.03446317e-08,  4.10027897e-08,  4.76656603e+01,  3.87562099e-08,\n",
       "       -2.34552766e-08,  7.37154089e-08, -1.98667078e-08,  3.99906820e-08,\n",
       "        7.14119677e-09, -8.24211968e-09,  5.90451699e-09,  5.36791743e+01,\n",
       "        2.74107728e-09,  3.08838189e-08,  5.80480087e-08, -2.44412679e-08,\n",
       "       -2.15125759e-09, -3.02420116e-08, -1.79914599e-08,  2.93136495e-08,\n",
       "        3.99585003e-08,  1.61651712e-08,  1.25024880e-08, -2.43632477e-08,\n",
       "        9.39177459e+01,  2.02122184e-11, -1.34573009e-08,  1.15842966e-08,\n",
       "       -9.68726552e-09, -2.78362309e-08, -3.80651014e-08, -1.31614096e-08,\n",
       "        3.42693652e-08,  2.19667906e-08, -1.53769486e-09, -3.17984752e-09,\n",
       "        6.44657250e-08, -1.48838984e-08, -9.32880998e-09, -7.67946729e-08,\n",
       "        6.03090280e-08, -4.86891894e-08, -2.72341963e-09, -2.26527563e-09,\n",
       "        1.13333812e+01, -5.20384350e-09,  4.46922501e-09,  2.08939601e-08,\n",
       "        2.02585679e-08, -2.77105230e-08,  7.72902746e-09,  7.97218767e+01,\n",
       "       -9.65870984e-09,  4.18368794e+01, -9.41487591e-09, -4.58211235e-08,\n",
       "        1.05382386e-08,  6.39892599e-09,  3.06175517e-09, -9.75699184e-09,\n",
       "        3.32447374e-08, -1.30483351e-08,  1.51279041e-08,  6.10823161e-09,\n",
       "        2.83601427e+01,  4.66792169e-08, -3.05215585e-08,  1.91004863e-08,\n",
       "       -2.76401823e-08, -1.24895613e-08, -5.32678090e-08,  1.38111586e-08,\n",
       "        2.27776162e-08,  3.09420210e-09, -5.66204636e-09,  1.73938495e-08,\n",
       "       -1.84836802e-08, -9.20535678e-09,  5.49395758e-08, -3.00940842e-08,\n",
       "       -1.74942418e-08,  3.14880500e-08,  5.71780742e-08,  9.27463198e-09,\n",
       "        1.57876046e-08,  1.00348420e-08, -5.21472863e-08, -2.44609644e-08,\n",
       "        1.65227846e-08,  7.78468503e-09,  1.25941597e-08,  1.47450201e-08])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
    "\n",
    "**LinearRegression** использует аналитический МНК, который находит точные оптимальные веса (коэффициенты) для линейной модели. Он минимизирует MSE сразу для всех данных.\n",
    "\n",
    "**SGDRegressor** использует SGD, который обновляет веса по шагам, используя небольшие подвыборки данных на каждой итерации. Это более приближённый и шумный метод, и потому итоговые коэффициенты могут не быть такими точными, либо наоборот быть точнее. На это влияет learning ratе, который может быть слишком большим или маленьким. Еще на это влияет alpha, которая добавляет штраф на слишком большие коэффициенты. Ну и, из-за стохастической природы метода, возможны колебания и случайные влияния на процесс обучения.\n",
    "\n",
    "SGDRegressor обновляет веса на каждой итерации, используя случайный объект или небольшую подвыборку, что делает процесс обучения \"шумным\" и зависит от рандомизации. В отличие от этого, LinearRegression использует все данные сразу, вычисляя точные значения весов на основе производных по всем объектам и признакам, что делает его результат стабильным и оптимальным для данной выборки.\n",
    "\n",
    "Результаты могут меняться в зависимости от выборки. У меня сейчас LinearRegression показывает лучшие результаты, хотя и было несколько ситуаций, когда SGDRegressor обучался лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq1076z-9Q-8"
   },
   "source": [
    "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8729183228513727e-25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.28811909e-16,  9.70832856e+01,  5.90071546e-15,  4.39528813e+01,\n",
       "       -2.45068066e-15,  2.60024984e-15,  6.21926361e+00,  1.97849990e-14,\n",
       "        7.86490496e-16,  1.27794454e-14,  4.76656608e+01, -1.09572568e-14,\n",
       "       -1.14312276e-14, -5.61628001e-15, -3.24246119e-14,  3.44416714e-15,\n",
       "        1.15483573e-14, -1.18909587e-14,  1.29059177e-14,  5.36791748e+01,\n",
       "       -6.22014116e-15,  1.14508609e-14,  3.98745958e-15,  4.14081979e-15,\n",
       "        3.14608581e-15,  1.55136714e-15, -1.59252653e-14, -3.65069219e-15,\n",
       "        7.83109870e-15, -1.44202258e-14, -1.64116292e-14, -6.26844733e-15,\n",
       "        9.39177468e+01,  3.71301135e-15, -1.48512612e-14, -1.67708297e-14,\n",
       "       -1.14895138e-14,  5.66043946e-15,  1.11497972e-14, -1.95501449e-14,\n",
       "        6.93729439e-15, -3.91858568e-15, -7.59083261e-15,  4.31607216e-16,\n",
       "        1.50002830e-16, -6.86865939e-15, -1.25708476e-14, -1.43334106e-14,\n",
       "        2.12917214e-14, -2.39964218e-15,  1.11138356e-15,  5.14903184e-15,\n",
       "        1.13333814e+01, -2.78802204e-15, -3.19036568e-15, -2.28861517e-15,\n",
       "       -5.25343506e-15,  1.07972666e-15, -8.13161806e-15,  7.97218776e+01,\n",
       "        5.49211690e-15,  4.18368798e+01, -7.12530834e-15,  2.26105972e-15,\n",
       "        2.32914151e-15,  9.80381886e-15,  8.26417781e-15,  1.24576618e-15,\n",
       "       -1.44680409e-14,  1.43686365e-15,  1.37071258e-14,  3.57451814e-15,\n",
       "        2.83601430e+01, -3.86488520e-15, -2.60486865e-14, -6.28932404e-15,\n",
       "       -7.02273415e-15, -7.18585889e-15, -7.53960583e-15, -2.31524047e-14,\n",
       "        1.34264059e-14, -4.85971050e-15, -1.65319926e-14,  1.62044349e-14,\n",
       "       -1.31562630e-14, -5.05654883e-15,  1.38588176e-14,  2.80306012e-15,\n",
       "       -1.14318157e-14, -2.33963529e-16, -2.45943760e-15, -1.21182415e-14,\n",
       "        2.02783804e-14, -1.71081844e-15, -1.54350827e-14,  1.08313184e-14,\n",
       "        6.65567750e-15, -4.55121460e-15, -4.10161954e-15, -5.82796214e-15])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# я тут сильно много вариантов пробовала, вроде это самый близкий результат дает\n",
    "reg = SGDRegressor(\n",
    "    alpha = 1e-14,                 # регуляризация стала больше\n",
    "    max_iter=1000,                 # количество итераций оставляю как по умолчано\n",
    "    learning_rate='constant',      # шаг обучения всегда один и тот же\n",
    "    eta0=0.001)                    # начальный размер шага          \n",
    "reg.fit(X, y)\n",
    "\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jJplHqS9Q-9"
   },
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKeXXhEH9Q--"
   },
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс.\n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь есть части, которые написал chat gpt. Не могу конкретно сказать, где, так как я задавала тысячу вопросов ему, так как у меня получался гигантский МСЕ (больше 10к). Но я теперь все понимаю (ну мне так кажется, что понимаю). Ниже мой исходный код:\n",
    "\n",
    ">>>\n",
    "class LinearRegressionGD:\n",
    "\n",
    ">>> def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
    "        self.alpha=alpha\n",
    "        self.tol=tol\n",
    "        self.max_iter=max_iter                     \n",
    "\n",
    ">>> def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  \n",
    "        self.theta = np.zeros(X_b.shape[1])        \n",
    "\n",
    ">>>     for i in range(self.max_iter):\n",
    ">>>         gradients = 2 / X_b.shape[0] * X_b.T.dot(X_b.dot(self.theta) - y)  \n",
    ">>>         new_theta = self.theta - self.alpha * gradients \n",
    "\n",
    ">>>         if np.linalg.norm(new_theta - self.theta) < self.tol:\n",
    ">>>             break\n",
    "\n",
    ">>>             self.theta = new_theta  \n",
    "\n",
    ">>> def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        return X_b.dot(self.theta)  \n",
    "\n",
    ">>> Пример использования\n",
    "\n",
    ">>> if __name__ == \"__main__\":\n",
    "    >>> X, y = make_regression(n_samples=10000, n_features=100, noise=0.1, random_state=42)\n",
    "\n",
    ">>>      model = LinearRegressionGD(l_ratio=0.001, max_iter=1000, tol=0.001)\n",
    ">>>      model.fit(X, y)\n",
    "\n",
    ">>>      predictions = model.predict(X)\n",
    "\n",
    ">>> mse = mean_squared_error(y, predictions)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "а этот мы написали с чатиком совмест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, alpha=0.01, tol=1e-6, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  \n",
    "        self.theta = np.zeros(X_b.shape[1])      \n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            gradients = 2 / X_b.shape[0] * X_b.T.dot(X_b.dot(self.theta) - y)  \n",
    "            new_theta = self.theta - self.alpha * gradients                   \n",
    "\n",
    "            # Критерий останова по норме разности весов\n",
    "            if np.linalg.norm(new_theta - self.theta, ord=2) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.theta = new_theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        return X_b.dot(self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = make_regression(n_samples=10000, n_features=100, noise=0.1, random_state=42)\n",
    "\n",
    "    model = LinearRegressionGD(alpha=0.01, max_iter=1000, tol=1e-6)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.01027350190221907\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, alpha=0.01, tol=1e-6, max_iter=1000, l1_ratio=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.l1_ratio = l1_ratio  # коэффициент регуляризации для L1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        self.theta = np.zeros(X_b.shape[1])      \n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            gradients = 2 / X_b.shape[0] * X_b.T.dot(X_b.dot(self.theta) - y)  \n",
    "            \n",
    "            # L1-регуляризация\n",
    "            l1_penalty = self.l1_ratio * np.sign(self.theta)\n",
    "            gradients += l1_penalty\n",
    "\n",
    "            new_theta = self.theta - self.alpha * gradients                   \n",
    "\n",
    "            if np.linalg.norm(new_theta - self.theta, ord=2) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.theta = new_theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        return X_b.dot(self.theta)  \n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = make_regression(n_samples=10000, n_features=100, noise=0.1, random_state=42)\n",
    "\n",
    "    model = LinearRegressionGD(alpha=0.01, max_iter=1000, tol=1e-6, l1_ratio=0.01)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m my_reg \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m      2\u001b[0m my_reg\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mean_squared_error(y, my_reg\u001b[38;5;241m.\u001b[39mpredict(X)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are amazing! Great work!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы с чатом не смогли исправить сутиацию, я меняла гиперпараметры очень много раз(( Я даже вставила код, который работает у подруги, но он не сработал у меня (это супер странно, так как у нее же регрессия рабочая) Короче, i haven't done any great work("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBRR_3Sh9Q_A"
   },
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error 0.1113104172641727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "print(f\"Mean Squared Error {mean_squared_error(y, lasso.predict(X))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03540174646595105\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionGD(alpha=0.01, max_iter=1000, tol=1e-6, l1_ratio=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Модель градиентного спуска дала более низкое значение MSE по сравнению с Lasso. Это указывает на то, что градиентный спуск обеспечил более точные прогнозы для данной выборки данных. Это может свидетельствовать о том, что наша реализация линейной регрессии с градиентным спуском более эффективно справляется с данной задачей регрессии, чем Lasso с заданным значением параметра регуляризации.\n",
    "\n",
    "Кроме того, применение градиентного спуска предоставляет дополнительные возможности для управления процессом обучения через выбор гиперпараметров и регуляризации. Так что, хотя Lasso является мощным инструментом для борьбы с переобучением и может быть более эффективным в других сценариях, в данной задаче градиентный спуск победил."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

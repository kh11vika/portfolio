{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae208a80-3d65-41f0-a12b-cb183d973c74",
   "metadata": {},
   "source": [
    "### Homework #1: Linear Regression (maximum 10 points)\n",
    "\n",
    "Some tasks will have different variants (there are 4 in total).\n",
    "To find out your variant, count the number of letters in your last name, take the remainder after dividing by 4, and add 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36059be1-351d-4ffe-a520-5728ade8712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname = \"Khachatryan\"\n",
    "letters_count = len(surname)\n",
    "variant = (letters_count % 4) + 1\n",
    "variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84755bb6-f371-41ef-a4f7-965bc58aa181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68488cfa-2c77-4958-b1fb-8c79f7b7095e",
   "metadata": {},
   "source": [
    "#### Multivariate Linear Regression from sklearn\n",
    "\n",
    "Let’s create a dataset for multivariate regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311a2ac5-90d1-40b3-be0c-f632d56e8f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491f56f-689c-440f-9022-1b1f3e75f910",
   "metadata": {},
   "source": [
    "We have 10,000 observations and 100 features.\n",
    "First, let’s solve the problem analytically “out of the box.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc5ab57-b7f8-42d4-a994-b8bacf7ff4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.517260350543703e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56651d9f-cae5-47d8-9cd1-0d5ef0d68289",
   "metadata": {},
   "source": [
    "Now let’s train linear regression using gradient descent, also “out of the box.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa0f3d6-d0a0-4ee2-858a-d8fedfcb250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.464757542693889e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.00705111e-08,  6.30153237e+01,  3.48735829e-08,  3.04924606e-09,\n",
       "        4.94863598e-08,  1.40682365e-08,  2.44767416e-08, -1.03822729e-08,\n",
       "       -5.30425967e-09, -3.85841742e-09,  5.25769499e-09, -7.94299818e-09,\n",
       "        3.11430378e-08,  5.44139079e-08,  8.39339973e+01,  6.13312826e-09,\n",
       "        1.31004561e-09, -5.29333391e-08, -4.15935652e-09,  1.64836351e-08,\n",
       "       -5.67348134e-09,  3.01089759e-08,  2.58541725e-08,  6.62792724e+01,\n",
       "       -5.96686586e-08,  8.33654736e+01,  3.75923050e-09,  5.33468026e-08,\n",
       "        3.07703726e-08, -3.99931084e-08,  8.38706540e-09,  1.22952100e-08,\n",
       "       -4.43324677e-08, -1.27071475e-08,  5.31720735e-08,  2.29093276e-08,\n",
       "       -8.39955047e-09,  6.38160414e-08,  6.75280726e-08, -3.35819450e-09,\n",
       "       -5.29295645e-09,  3.81422983e-08,  2.40703297e-08, -2.46318298e-09,\n",
       "        9.83993128e+01, -6.29936943e-09,  3.62150781e-08,  6.25566974e-08,\n",
       "        7.52658664e+00,  5.65939652e+01, -6.31275082e-09, -2.31639079e-08,\n",
       "        9.07083661e-09, -1.78693013e-09,  3.10327737e-08,  6.20905569e+01,\n",
       "       -2.07562220e-08, -3.54186816e-08, -5.30864071e-08, -8.23900919e-08,\n",
       "       -3.81082574e-08,  1.25419419e-07, -4.60420667e-08,  3.24396023e-08,\n",
       "       -5.89759532e-08, -1.20690017e-08,  1.77298254e-08, -3.75990743e-08,\n",
       "       -1.27161147e-09, -4.96149370e-08,  1.66489631e-08,  2.57834389e-08,\n",
       "       -1.71475050e-08,  7.80082622e-08, -7.21761011e-08,  1.25892362e-08,\n",
       "        3.36134777e-08,  6.27941221e+01, -3.18554423e-08,  3.64135228e-08,\n",
       "        9.99840753e-09,  3.35522019e-08, -4.47480239e-08,  1.37537237e-08,\n",
       "        2.22494296e-08, -5.02325039e-08,  2.13728699e-08, -4.15846281e-08,\n",
       "        4.49092059e+00,  3.01706065e-08, -1.42575424e-08, -5.76174212e-10,\n",
       "       -6.19965621e-08,  3.02539880e-08, -5.16056092e-08,  5.06795212e-08,\n",
       "       -1.11457335e-08, -1.13075501e-08,  4.21380735e-08, -2.87879241e-09])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549070a-807d-48d4-ba08-49c6d4e1f0d2",
   "metadata": {},
   "source": [
    "#### Task 1 (1 point)\n",
    "\n",
    "Explain why there is a difference between the two MSE values obtained.\n",
    "\n",
    "LinearRegression uses analytical OLS, which finds exact optimal weights (coefficients) for the linear model by minimizing MSE on the whole dataset.\n",
    "\n",
    "SGDRegressor uses stochastic gradient descent, which updates weights step by step using small data batches on each iteration.\n",
    "This is an approximate and noisier method, so the resulting coefficients might be less accurate (or sometimes more accurate, depending on randomness).\n",
    "The learning rate can also be too high or too low, affecting convergence.\n",
    "Additionally, the alpha parameter adds a penalty for large coefficients.\n",
    "Because of the stochastic nature of the method, the process may fluctuate depending on random initialization and sample order.\n",
    "\n",
    "Unlike LinearRegression, which computes exact weights using all data at once, SGDRegressor updates weights sequentially, which introduces randomness and may cause variation between runs.\n",
    "\n",
    "In my case, LinearRegression performs better, although in some runs SGDRegressor produced lower MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bc1d9-c0a0-4735-b4eb-0efc70f7aad8",
   "metadata": {},
   "source": [
    "#### Task 2 (1 point)\n",
    "\n",
    "Tune the hyperparameters of gradient descent so that the MSE value is close to that obtained with LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658d10b1-85a5-4f4e-9be6-b0457c7aebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.676095881190452e-25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.72215222e-14,  6.30153243e+01,  7.64638772e-15,  1.34819691e-14,\n",
       "       -4.02234865e-17, -1.04094146e-14,  1.57234496e-14, -1.36968636e-14,\n",
       "       -1.13847359e-14, -3.25267114e-14, -2.54187969e-15,  2.74503173e-15,\n",
       "        2.70146305e-15,  2.65315043e-14,  8.39339983e+01, -1.09583998e-14,\n",
       "       -1.02700313e-14, -6.53081467e-15, -2.93141706e-14, -2.00142969e-14,\n",
       "        9.15582152e-15,  5.77986090e-15,  1.54796365e-14,  6.62792731e+01,\n",
       "       -3.77909379e-14,  8.33654745e+01, -4.31592502e-15,  1.27500124e-16,\n",
       "        2.28885878e-14, -1.75751666e-15,  4.47360875e-15,  1.24668258e-14,\n",
       "       -1.24097430e-14, -2.03990656e-14, -1.01733905e-14, -1.37744957e-14,\n",
       "       -1.02653952e-14,  2.43652618e-15,  5.79282242e-15,  2.28014899e-14,\n",
       "        6.35821370e-15, -1.13771680e-14, -1.44598225e-15, -1.05201924e-14,\n",
       "        9.83993137e+01, -1.03812664e-14,  1.54788612e-14,  5.19949089e-15,\n",
       "        7.52658676e+00,  5.65939657e+01, -2.07395433e-14, -1.99351764e-14,\n",
       "       -9.86396008e-15,  4.60378368e-15, -1.02035796e-14,  6.20905575e+01,\n",
       "        1.88642217e-14, -2.07108408e-14,  5.91468079e-15, -2.59675744e-14,\n",
       "        1.42498296e-15,  1.75479287e-14,  8.54810099e-15, -1.49356270e-14,\n",
       "        1.70524377e-15,  1.03176739e-14,  1.64698183e-14, -1.39348376e-15,\n",
       "       -1.98075991e-14,  2.42857074e-14, -2.59275238e-15,  1.23081111e-14,\n",
       "       -7.11338448e-15,  4.78997809e-18,  7.93985166e-15,  1.69689331e-14,\n",
       "       -5.41238779e-15,  6.27941228e+01, -1.02323070e-14,  1.59256320e-14,\n",
       "        8.11780850e-15,  2.96093634e-14, -2.41418604e-14, -6.42059386e-15,\n",
       "        1.46645915e-15, -7.41120528e-15,  1.70256122e-14,  1.98184095e-14,\n",
       "        4.49092064e+00,  1.00293441e-15,  3.76097958e-15,  1.15439474e-14,\n",
       "        1.18820804e-14,  5.69767553e-15,  1.56680180e-15, -1.59659445e-14,\n",
       "       -5.66475309e-15, -9.41992672e-15,  2.40249906e-14, -5.25218242e-15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I tried many options — this seems to produce the closest result\n",
    "reg = SGDRegressor(\n",
    "    alpha = 1e-14,                 # stronger regularization\n",
    "    max_iter = 1000,               # leave iteration count as default\n",
    "    learning_rate = 'constant',    # constant learning rate\n",
    "    eta0 = 0.001                   # initial learning step size\n",
    ")\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89c0dd-a4ab-47eb-81b4-b2a1530f6a62",
   "metadata": {},
   "source": [
    "#### Your Own Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac01d28-9f8b-4e03-9ea8-4cea38a709ab",
   "metadata": {},
   "source": [
    "# I used ChatGPT to optimize the code here :)\n",
    "# I used to calculate it with a loop over features\n",
    "class MyLinearRegression:\n",
    "    def fit(self, X, y):\n",
    "        # add column of ones for intercept\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        # analytical solution using normal equations\n",
    "        self.coef_ = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b16b2-d1ab-49cb-a9d4-624315e6e6bd",
   "metadata": {},
   "source": [
    "Now check how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67939ffd-5f56-457f-aaee-0507da656c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5123811161210144e-26\n"
     ]
    }
   ],
   "source": [
    "my_reg = MyLinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "print(mean_squared_error(y, my_reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0969dd-67f7-4ca0-9886-35937929d67d",
   "metadata": {},
   "source": [
    "#### Task 3 (1 point)\n",
    "\n",
    "Compare the coefficients of your model with those from LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "471766fc-bb26-4250-a626-2e7e1497e2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression().fit(X, y)\n",
    "\n",
    "np.allclose(\n",
    "    my_reg.coef_,\n",
    "    np.r_[lin_reg.intercept_, lin_reg.coef_]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c10bb7-0c9a-4231-b049-536d411d8cf1",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Our implementation is mathematically equivalent to LinearRegression because both use the same analytical ordinary least-squares solution via normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ae26d-ffcf-43a9-86fa-5c32d89b658f",
   "metadata": {},
   "source": [
    "#### Task 4 (1 point)\n",
    "\n",
    "Add normalization of features before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f518706f-9b79-48e1-95f9-27ce4d619444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.470862475281235e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "reg_scaled = LinearRegression().fit(X_scaled, y)\n",
    "print(mean_squared_error(y, reg_scaled.predict(X_scaled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9e084-94c4-4b22-afaa-b9bcbf4a9d1d",
   "metadata": {},
   "source": [
    "#### Task 5 (1 point)\n",
    "\n",
    "Implement normalization inside your own class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2680a01-4df3-42ca-b773-2b068bab3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegressionScaled:\n",
    "    def fit(self, X, y):\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.std_ = X.std(axis=0)\n",
    "        X_scaled = (X - self.mean_) / self.std_\n",
    "        X_b = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "        self.coef_ = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_scaled = (X - self.mean_) / self.std_\n",
    "        X_b = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "        return X_b @ self.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0eedfe7-cf74-4bcd-ba78-86f5ca36e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2328312664263254e-26\n"
     ]
    }
   ],
   "source": [
    "reg_my_scaled = MyLinearRegressionScaled()\n",
    "reg_my_scaled.fit(X, y)\n",
    "print(mean_squared_error(y, reg_my_scaled.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bda511-1b98-45df-b7ce-82b7ecbb7690",
   "metadata": {},
   "source": [
    "#### Task 6 (1 point)\n",
    "\n",
    "Compare your normalized model with the one from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256f5871-6755-4c0d-935b-456ce798c2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_sk_scaled = LinearRegression().fit(X_scaled, y)\n",
    "np.allclose(\n",
    "    reg_my_scaled.coef_,\n",
    "    np.r_[reg_sk_scaled.intercept_, reg_sk_scaled.coef_]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0d493-4cf8-4394-bdd9-c4484c2f106d",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Normalization doesn’t change model quality but makes optimization more stable and interpretable — all coefficients are on a comparable scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38658ea3-9ed2-4bc1-8012-9798eebb5085",
   "metadata": {},
   "source": [
    "#### Task 7 (1 point)\n",
    "\n",
    "Generate a dataset with noise and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40163238-b83e-48fb-a0f5-1a9e26b88c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2321.882005350111\n"
     ]
    }
   ],
   "source": [
    "X_noisy, y_noisy = make_regression(\n",
    "    n_samples=1000, n_features=10, noise=50, random_state=42\n",
    ")\n",
    "reg_noise = LinearRegression().fit(X_noisy, y_noisy)\n",
    "print(mean_squared_error(y_noisy, reg_noise.predict(X_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e376e4-e68f-48c4-b2ab-71c3bfe51ed5",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Adding noise increases MSE because the target variable now contains random components that the model can’t explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1725e3-4f99-490b-812a-72806ee4f88d",
   "metadata": {},
   "source": [
    "#### Task 8 (1 point)\n",
    "Add l1 (for the first and fourth variants) or l2 (for the second and third variants) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c32df16-ddcd-4233-a96e-6cac6a41dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.01027350190221907\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, alpha=0.01, tol=1e-6, max_iter=1000, l1_ratio=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.l1_ratio = l1_ratio \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        self.theta = np.zeros(X_b.shape[1])      \n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            gradients = 2 / X_b.shape[0] * X_b.T.dot(X_b.dot(self.theta) - y)  \n",
    "            \n",
    "            l1_penalty = self.l1_ratio * np.sign(self.theta)\n",
    "            gradients += l1_penalty\n",
    "\n",
    "            new_theta = self.theta - self.alpha * gradients                   \n",
    "\n",
    "            if np.linalg.norm(new_theta - self.theta, ord=2) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.theta = new_theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        return X_b.dot(self.theta)  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = make_regression(n_samples=10000, n_features=100, noise=0.1, random_state=42)\n",
    "\n",
    "    model = LinearRegressionGD(alpha=0.01, max_iter=1000, tol=1e-6, l1_ratio=0.01)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e612ce-46e2-4609-9f21-c731592cd265",
   "metadata": {},
   "source": [
    "#### Task 9 (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d8c4756-2557-41a8-8de3-470c6aff68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error 0.1113104172641727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "print(f\"Mean Squared Error {mean_squared_error(y, lasso.predict(X))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295ed10-b738-47b0-8487-5e23933eb9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
